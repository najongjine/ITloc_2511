https://colab.research.google.com/drive/19XYbgDLlHNpTj7bNiK-gcYwZAnMuq_qC?usp=sharing
- GPT ë§Œë“¤ê¸° ì‹¤ìŠµ
colab ëŸ°íƒ€ì„ T4,
input.txt ì˜¬ë¦¬ê¸°
ëª¨ë‘ì‹¤í–‰ í´ë¦­



# --------í›ˆë ¨ ì•ˆ ëœ ëª¨ë¸ (ìŒ©ìœ¼ë¡œ ëŒë¦¬ê¸°)-------


import torch
import torch.nn.functional as F
import tiktoken

# 1. í† í¬ë‚˜ì´ì € ì¤€ë¹„
enc = tiktoken.get_encoding("cl100k_base")

# 2. ë¬¸ì¥ ìƒì„± í•¨ìˆ˜ (Generate)
def generate(model, start_text, max_new_tokens=50, temperature=1.0):
    model.eval()
    # ì…ë ¥ í…ìŠ¤íŠ¸ -> í† í° ID ë³€í™˜
    start_ids = enc.encode(start_text)
    idx = torch.tensor(start_ids, dtype=torch.long, device=config.device).unsqueeze(0)

    print(f"ì…ë ¥: {start_text}")
    print("-" * 30)

    for _ in range(max_new_tokens):
        # context window(block_size)ë¥¼ ë„˜ì§€ ì•Šê²Œ ìë¥´ê¸°
        idx_cond = idx if idx.size(1) <= config.block_size else idx[:, -config.block_size:]
        
        # ëª¨ë¸ ì˜ˆì¸¡
        logits, _ = model(idx_cond)
        logits = logits[:, -1, :] / temperature
        probs = F.softmax(logits, dim=-1)
        
        # ìƒ˜í”Œë§
        idx_next = torch.multinomial(probs, num_samples=1)
        idx = torch.cat((idx, idx_next), dim=1)

    return enc.decode(idx[0].tolist())

# --- ì‹¤í–‰ ë¶€ë¶„ ---
print("ğŸ”µ [Scenario 1] í›ˆë ¨ë˜ì§€ ì•Šì€ Raw ëª¨ë¸ ì‹¤í–‰ ì¤‘...")

# ëª¨ë¸ ì´ˆê¸°í™” (ëœë¤ ê°€ì¤‘ì¹˜)
raw_model = GPT().to(config.device)

# ìƒì„± í…ŒìŠ¤íŠ¸
output = generate(raw_model, "Hello", max_new_tokens=30)
print(f"ê²°ê³¼: {output}")

# --------í›ˆë ¨ ì•ˆ ëœ ëª¨ë¸ (ìŒ©ìœ¼ë¡œ ëŒë¦¬ê¸°) END ------- 




# --- í›ˆë ¨ëœ ëª¨ë¸ ë¶ˆëŸ¬ì™€ì„œ ëŒë¦¬ê¸° ---

import os
from google.colab import drive

# --- ì‹¤í–‰ ë¶€ë¶„ ---
print("ğŸŸ  [Scenario 2] êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ í•™ìŠµëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°...")

# 1. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸
drive.mount('/content/drive')

# 2. ëª¨ë¸ ì´ˆê¸°í™” ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
save_path = '/content/drive/MyDrive/My_GPT_Models/gpt2_small_custom.pth' # ì €ì¥í–ˆë˜ ê²½ë¡œ í™•ì¸

if os.path.exists(save_path):
    trained_model = GPT().to(config.device)
    
    # ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë®ì–´ì“°ê¸°
    trained_model.load_state_dict(torch.load(save_path, map_location=config.device))
    print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
    
    # 3. ìƒì„± í…ŒìŠ¤íŠ¸
    output = generate(trained_model, "who are you?", max_new_tokens=50) # ìœ„ì—ì„œ ì •ì˜í•œ generate í•¨ìˆ˜ ì¬ì‚¬ìš©
    print(f"ê²°ê³¼: {output}")
else:
    print(f"âŒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {save_path}")

# --- í›ˆë ¨ëœ ëª¨ë¸ ë¶ˆëŸ¬ì™€ì„œ ëŒë¦¬ê¸° END ---


==============================
   ğŸš€ X-Ray ì§„ë‹¨ ëª¨ë“œ ì‹¤í–‰   
==============================

=== [X-Ray] 1. ì…ë ¥ ë‹¨ê³„ ===
[Input Tokens] Shape: [1, 2] | ì…ë ¥ëœ í† í° IDë“¤

=== [X-Ray] 2. ì„ë² ë”© ë‹¨ê³„ ===
[Embeddings] Shape: [1, 2, 512] | í† í°ì´ 512ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜ë¨
[Layer 0 ì™„ë£Œ] Shape: [1, 2, 512] | ë¸”ë¡ í•˜ë‚˜(Attn+FF) í†µê³¼ í›„ ë°ì´í„° ëª¨ì–‘

=== [X-Ray] 3. ìµœì¢… ì¶œë ¥ ë‹¨ê³„ ===
[Logits] Shape: [1, 1, 100277] | ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ ì ìˆ˜ (Vocab Size: 100277)